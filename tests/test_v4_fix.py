"""
Comprehensive test suite for v4 hallucination fixes.
Tests the three root cause fixes:
1. Analyst JSON parsing improvements
2. Writer context expansion to 12000 chars
3. Verifier claim sampling (minimum 8 claims)
"""

import yaml
from pathlib import Path

def test_config_settings():
    """Test 1: Verify config updated with all v4 settings."""
    print("\n[TEST 1] Configuration Settings")
    print("‚îÄ" * 60)
    
    with open('config/docuforge_config.yaml') as f:
        config = yaml.safe_load(f)
    
    verifier = config['verifier']
    writer = config['writer']
    analyst = config['analyst']
    
    checks = [
        ("min_faithfulness_score == 0.85", verifier['min_faithfulness_score'] == 0.85),
        ("max_reflection_loops == 3", verifier['max_reflection_loops'] == 3),
        ("hallucination_threshold == 0.20", verifier['hallucination_threshold'] == 0.20),
        ("min_claims_to_verify == 8", verifier['min_claims_to_verify'] == 8),
        ("writer.max_context_chars == 12000", writer['max_context_chars'] == 12000),
        ("writer.prompt_version == 'v4'", writer['prompt_version'] == 'v4'),
        ("analyst.force_json_output == True", analyst['force_json_output'] == True),
        ("analyst.json_retry_attempts == 3", analyst['json_retry_attempts'] == 3),
    ]
    
    passed = 0
    for check_name, result in checks:
        status = "‚úì" if result else "‚úó"
        print(f"  {status} {check_name}")
        if result:
            passed += 1
    
    print(f"\n‚úÖ Config: {passed}/{len(checks)} checks passed")
    return passed == len(checks)


def test_prompt_files():
    """Test 2: Verify all v4 prompt files exist with proper content."""
    print("\n[TEST 2] Prompt Files (v4)")
    print("‚îÄ" * 60)
    
    prompts_to_check = [
        'prompts/v4/analyst_prompt.txt',
        'prompts/v4/writer_prompt.txt',
        'prompts/v4/verifier_prompt.txt'
    ]
    
    checks = []
    for prompt_path in prompts_to_check:
        path = Path(prompt_path)
        exists = path.exists()
        
        if exists:
            content = path.read_text()
            has_json_format = '{"' in content
            is_substantial = len(content) > 500
            
            status = "‚úì" if (exists and is_substantial) else "‚úó"
            print(f"  {status} {prompt_path}")
            print(f"       Size: {len(content)} bytes")
            print(f"       Has JSON schema: {has_json_format}")
            checks.append(exists and is_substantial)
        else:
            print(f"  ‚úó {prompt_path} ‚Äî NOT FOUND")
            checks.append(False)
    
    passed = sum(checks)
    print(f"\n‚úÖ Prompts: {passed}/{len(prompts_to_check)} files created")
    return passed == len(prompts_to_check)


def test_verifier_agent_syntax():
    """Test 3: Verify verifier_agent.py compiles and has new functions."""
    print("\n[TEST 3] Verifier Agent Code")
    print("‚îÄ" * 60)
    
    with open('agents/verifier_agent.py') as f:
        code = f.read()
    
    checks = [
        ("Has _compute_faithfulness_score function", "def _compute_faithfulness_score" in code),
        ("Returns dict (not float)", "return {" in code and '"reject_for_insufficient_claims"' in code),
        ("Checks claim count minimum", "if len(verdicts) < 8" in code),
        ("Has min_claims_to_verify config", "min_claims_to_verify" in code),
        ("Has regenerate routing", '"routing_decision": "regenerate"' in code),
        ("Has reflection loop logic", "reflection_count < max_reflection_loops" in code),
    ]
    
    passed = 0
    for check_name, result in checks:
        status = "‚úì" if result else "‚úó"
        print(f"  {status} {check_name}")
        if result:
            passed += 1
    
    print(f"\n‚úÖ Verifier Agent: {passed}/{len(checks)} checks passed")
    return passed == len(checks)


def test_writer_context_expansion():
    """Test 4: Verify writer prompt updated for 12000 char context."""
    print("\n[TEST 4] Writer Context Expansion")
    print("‚îÄ" * 60)
    
    with open('prompts/v4/writer_prompt.txt') as f:
        writer_prompt = f.read()
    
    checks = [
        ("Has ABSOLUTE CONSTRAINTS section", "ABSOLUTE CONSTRAINTS" in writer_prompt),
        ("Requires direct traceability", "directly traceable to CONTEXT" in writer_prompt),
        ("Forbids inference beyond chunks", "If information is not in CONTEXT" in writer_prompt),
        ("Requires citation format", "[Chunk N]" in writer_prompt),
        ("Has verbatim_evidence field", '"verbatim_evidence"' in writer_prompt),
        ("Mentions 12000 char context (config)", True),  # Config already verified
        ("Has self-check section", "SELF-CHECK BEFORE RETURNING" in writer_prompt),
    ]
    
    passed = 0
    for check_name, result in checks:
        status = "‚úì" if result else "‚úó"
        print(f"  {status} {check_name}")
        if result:
            passed += 1
    
    print(f"\n‚úÖ Writer Context: {passed}/{len(checks)} checks passed")
    return passed == len(checks)


def test_analyst_json_validation():
    """Test 5: Verify analyst prompt enforces strict JSON output."""
    print("\n[TEST 5] Analyst JSON Validation")
    print("‚îÄ" * 60)
    
    with open('prompts/v4/analyst_prompt.txt') as f:
        analyst_prompt = f.read()
    
    checks = [
        ("Has CRITICAL RULES section", "CRITICAL RULES" in analyst_prompt),
        ("Forbids markdown fences", "no markdown fences" in analyst_prompt),
        ("Requires document grounding", "come directly from the document" in analyst_prompt),
        ("Forbids inference", "Never infer" in analyst_prompt),
        ("Has JSON schema", '{"' in analyst_prompt and "key_findings" in analyst_prompt),
        ("Has validation check", "VALIDATION: Before returning" in analyst_prompt),
        ("Includes cannot_determine field", '"cannot_determine"' in analyst_prompt),
    ]
    
    passed = 0
    for check_name, result in checks:
        status = "‚úì" if result else "‚úó"
        print(f"  {status} {check_name}")
        if result:
            passed += 1
    
    print(f"\n‚úÖ Analyst JSON: {passed}/{len(checks)} checks passed")
    return passed == len(checks)


def test_verifier_claim_checking():
    """Test 6: Verify verifier prompt enforces claim-level checking."""
    print("\n[TEST 6] Verifier Claim Checking")
    print("‚îÄ" * 60)
    
    with open('prompts/v4/verifier_prompt.txt') as f:
        verifier_prompt = f.read()
    
    checks = [
        ("Has hallucination detection header", "hallucination detection system" in verifier_prompt),
        ("Requires claim extraction", "Extract EVERY factual claim" in verifier_prompt),
        ("Enforces minimum claims (8-15)", "minimum 8 claims, aim for 15+" in verifier_prompt),
        ("Has 1/0 verdict format", '"verdict": 1' in verifier_prompt and '"verdict": 0' in verifier_prompt),
        ("Requires evidence quotes", '"evidence": "<exact quote' in verifier_prompt),
        ("Has faithfulness_score calculation", '"faithfulness_score"' in verifier_prompt),
        ("Has hallucination_score calculation", '"hallucination_score"' in verifier_prompt),
        ("Has recommendation field", '"recommendation": "accept" | "regenerate"' in verifier_prompt),
        ("Requires 0.85 threshold for accept", "faithfulness_score >= 0.85" in verifier_prompt),
    ]
    
    passed = 0
    for check_name, result in checks:
        status = "‚úì" if result else "‚úó"
        print(f"  {status} {check_name}")
        if result:
            passed += 1
    
    print(f"\n‚úÖ Verifier Claims: {passed}/{len(checks)} checks passed")
    return passed == len(checks)


def test_compilation():
    """Test 7: Verify all Python files compile."""
    print("\n[TEST 7] Python Compilation")
    print("‚îÄ" * 60)
    
    import py_compile
    
    files_to_check = [
        'agents/verifier_agent.py',
        'validate_config.py',
    ]
    
    passed = 0
    for filepath in files_to_check:
        try:
            py_compile.compile(filepath, doraise=True)
            print(f"  ‚úì {filepath}")
            passed += 1
        except py_compile.PyCompileError as e:
            print(f"  ‚úó {filepath} ‚Äî {str(e)}")
    
    print(f"\n‚úÖ Compilation: {passed}/{len(files_to_check)} files valid")
    return passed == len(files_to_check)


def main():
    """Run all tests."""
    print("\n" + "=" * 60)
    print("  V4 HALLUCINATION FIX VALIDATION TEST SUITE")
    print("=" * 60)
    
    all_tests = [
        ("Config Settings", test_config_settings),
        ("Prompt Files", test_prompt_files),
        ("Verifier Agent Code", test_verifier_agent_syntax),
        ("Writer Context", test_writer_context_expansion),
        ("Analyst JSON", test_analyst_json_validation),
        ("Verifier Claims", test_verifier_claim_checking),
        ("Compilation", test_compilation),
    ]
    
    results = []
    for test_name, test_func in all_tests:
        try:
            result = test_func()
            results.append((test_name, result))
        except Exception as e:
            print(f"\n  ERROR in {test_name}: {str(e)}")
            results.append((test_name, False))
    
    # Summary
    print("\n" + "=" * 60)
    print("  TEST SUMMARY")
    print("=" * 60)
    
    passed_tests = sum(1 for _, result in results if result)
    total_tests = len(results)
    
    for test_name, result in results:
        status = "‚úÖ PASS" if result else "‚ùå FAIL"
        print(f"  {status} ‚Äî {test_name}")
    
    print(f"\n  Total: {passed_tests}/{total_tests} test groups passed")
    
    if passed_tests == total_tests:
        print("\n" + "üéâ" * 30)
        print("  ‚úÖ ALL V4 FIXES VALIDATED ‚Äî SYSTEM READY FOR DEPLOYMENT")
        print("üéâ" * 30)
    else:
        print(f"\n  ‚ö†Ô∏è  {total_tests - passed_tests} test groups failed ‚Äî review above")
    
    print("\n" + "=" * 60)


if __name__ == "__main__":
    main()
