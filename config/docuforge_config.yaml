llm:
  # Fallback chain: Groq (14,400 req/day free) → Gemini → Ollama (unlimited, local)
  # Set GROQ_API_KEY in .env for best free tier quota
  # Groq models: llama-3.1-8b-instant, llama-3.3-70b-versatile, mixtral-8x7b-32768
  # Gemini models (fallback): gemini-2.0-flash, gemini-1.5-flash-latest, etc
  primary_model: "gemini-2.0-flash"
  fallback_model: "llama3.2"
  temperature: 0.2
  max_tokens: 4096

rag:
  chunk_size: 400
  chunk_overlap: 80
  top_k_results: 20
  semantic_weight: 0.7
  keyword_weight: 0.3
  embedding_model: "sentence-transformers/all-MiniLM-L6-v2"

verifier:
  min_faithfulness_score: 0.85
  max_reflection_loops: 3
  hallucination_threshold: 0.20
  min_claims_to_verify: 8

writer:
  max_context_chars: 12000
  prompt_version: "v4"

analyst:
  force_json_output: true
  json_retry_attempts: 3

ingestion:
  supported_formats:
    - pdf
    - png
    - jpg
    - jpeg
    - mp3
    - wav
    - xlsx
    - pptx
  max_file_size_mb: 50

eval:
  golden_dataset_path: "eval/test_dataset.json"
  min_accuracy_threshold: 0.80
  bias_similarity_threshold: 0.95

api:
  host: "0.0.0.0"
  port: 8000
  workers: 4

celery:
  broker_url: "${UPSTASH_REDIS_URL}"
  result_backend: "${UPSTASH_REDIS_URL}"
