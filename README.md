# DocuForge AI

**Intelligent Multi-Agent Document Intelligence Platform**

> Upload any document. 7 specialized AI agents collaborate to analyze, retrieve, reason, and generate a verified, hallucination-controlled report â€” in under 60 seconds.

[![Python](https://img.shields.io/badge/Python-3.11+-blue)](https://python.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.110+-green)](https://fastapi.tiangolo.com)
[![LangGraph](https://img.shields.io/badge/LangGraph-0.2+-orange)](https://langchain-ai.github.io/langgraph)
[![React](https://img.shields.io/badge/React-18+-61DAFB)](https://react.dev)
[![License](https://img.shields.io/badge/License-MIT-yellow)](LICENSE)

---


## What is DocuForge AI?

DocuForge AI is a production-grade multi-agent document intelligence system built to demonstrate advanced AI engineering skills. It accepts any document format and deploys a coordinated pipeline of 7 AI agents that parse, retrieve, analyze, write, and verify a structured report â€” with built-in hallucination control and full observability.

**Built for:** Agentic AI Engineer interviews, AI/ML Engineer roles, LLM application portfolios

---

## Demo

[![DocuForge AI Demo](https://img.youtube.com/vi/iMASmk7Rky8/maxresdefault.jpg)](https://youtu.be/iMASmk7Rky8)

> Click the thumbnail above to watch the full demo â€” live document upload, 7 agents executing in real time, 
> ReAct pattern visualization, prompt versioning, and hallucination scoring.

---

## Architecture

```
User Upload (PDF/PNG/JPG/MP3/WAV/XLSX/PPTX)
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              LangGraph StateGraph               â”‚
â”‚                                                 â”‚
â”‚  ðŸ§­ Supervisor â†’ decides routing at each step  â”‚
â”‚        â”‚                                        â”‚
â”‚        â”œâ”€â”€â–¶ ðŸ“¥ Ingestion Agent                 â”‚
â”‚        â”‚       PyMuPDF / Gemini Vision /        â”‚
â”‚        â”‚       Whisper / pandas / pptx          â”‚
â”‚        â”‚                                        â”‚
â”‚        â”œâ”€â”€â–¶ ðŸ“š RAG Agent                       â”‚
â”‚        â”‚       Hybrid BM25 + Semantic Search    â”‚
â”‚        â”‚       ChromaDB + HuggingFace Embeddingsâ”‚
â”‚        â”‚                                        â”‚
â”‚        â”œâ”€â”€â–¶ ðŸŒ Research Agent (optional)       â”‚
â”‚        â”‚       DuckDuckGo web search            â”‚
â”‚        â”‚                                        â”‚
â”‚        â”œâ”€â”€â–¶ ðŸ“Š Analyst Agent                   â”‚
â”‚        â”‚       Groq LLaMA + Code Executor       â”‚
â”‚        â”‚                                        â”‚
â”‚        â”œâ”€â”€â–¶ âœï¸  Writer Agent                   â”‚
â”‚        â”‚       ReAct-grounded prompt (v1/v2/v3) â”‚
â”‚        â”‚       Anti-hallucination rules         â”‚
â”‚        â”‚                                        â”‚
â”‚        â””â”€â”€â–¶ ðŸ›¡ï¸  Verifier Agent               â”‚
â”‚                Claim-level faithfulness scoring â”‚
â”‚                Reflection loop (max 1 retry)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
React Dashboard (Real-time agent visualization)
â”œâ”€â”€ Report Tab      â€” Structured sections with confidence bars
â”œâ”€â”€ ReAct Pattern   â€” Thought / Action / Observation per agent
â”œâ”€â”€ Prompt Versions â€” v1 / v2 / v3 selectable before analysis
â””â”€â”€ Metrics Tab     â€” Faithfulness %, Hallucination %, Stage checklist
```

---

## Key Features

| Feature | Implementation |
|---|---|
| **Multi-agent orchestration** | LangGraph StateGraph with 7 nodes + conditional routing |
| **Hallucination control** | Claim-level verifier scoring + anti-hallucination prompt rules |
| **Hybrid RAG** | BM25 keyword + semantic vector search with ensemble weighting |
| **Multimodal ingestion** | PDF, PNG/JPG (Gemini Vision), MP3/WAV (Whisper), XLSX, PPTX |
| **Prompt versioning** | v1/v2/v3 selectable per request, visible in UI |
| **ReAct pattern** | Every agent exposes Thought/Action/Observation in UI |
| **Background processing** | Celery + Upstash Redis for async pipeline execution |
| **Real-time UI** | React + ReactFlow agent graph with live trace polling |
| **Observability** | LangSmith tracing (optional), full agent trace in UI |
| **Memory systems** | Short-term (session), long-term (Supabase), episodic (ChromaDB) |
| **Zero cost** | Groq free tier + local embeddings + Upstash free Redis |

---

## Tech Stack

**Backend**
- Python 3.11+, FastAPI, LangChain, LangGraph
- Groq (LLaMA 3.1) â€” primary LLM, free tier
- Google Gemini 1.5 Flash â€” multimodal fallback
- ChromaDB â€” vector store (in-memory per session)
- HuggingFace sentence-transformers/all-MiniLM-L6-v2 â€” local embeddings
- Celery + Upstash Redis â€” async task queue
- Whisper base â€” audio transcription

**Frontend**
- React 18, Vite, ReactFlow
- Tailwind CSS, Radix UI, lucide-react

**Deployment**
- Docker + docker-compose
- FastAPI + Uvicorn
- PostgreSQL (Supabase) for session storage
- Redis (Upstash) for task queue

---

## Quickstart

### Prerequisites
- Python 3.11+
- Node.js 18+
- Groq API key (free at [console.groq.com](https://console.groq.com))
- Upstash Redis URL (free at [upstash.com](https://upstash.com))

### Setup

```bash
# 1. Clone and create environment
git clone https://github.com/yourusername/docuforge-ai
cd docuforge-ai
python -m venv .venv
.venv\Scripts\activate  # Windows
# source .venv/bin/activate  # Mac/Linux

# 2. Install dependencies
pip install -r requirements.txt
cd frontend && npm install && cd ..

# 3. Configure environment
copy .env.example .env
# Edit .env and set:
# GROQ_API_KEY=your_groq_key
# UPSTASH_REDIS_URL=rediss://default:...@....upstash.io:6379
# GEMINI_API_KEY=your_gemini_key (optional, for image analysis)

# 4. Start everything
make start
# or: python run.py
```

Dashboard opens automatically at **http://localhost:3000**

---

## Project Structure

```
docuforge-ai/
â”œâ”€â”€ agents/                     # 7 specialized AI agents
â”‚   â”œâ”€â”€ supervisor_agent.py     # Query routing and pipeline control
â”‚   â”œâ”€â”€ ingestion_agent.py      # Document parsing orchestration
â”‚   â”œâ”€â”€ rag_agent.py            # Chunk + embed + retrieve
â”‚   â”œâ”€â”€ research_agent.py       # Optional web search
â”‚   â”œâ”€â”€ analyst_agent.py        # Numerical analysis + code execution
â”‚   â”œâ”€â”€ writer_agent.py         # ReAct-grounded report generation
â”‚   â””â”€â”€ verifier_agent.py       # Claim-level faithfulness scoring
â”‚
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ ingestion/              # Multimodal parsers (PDF/image/audio/xlsx/pptx)
â”‚   â”œâ”€â”€ rag/                    # Chunker, embedder, vectorstore, retriever
â”‚   â”œâ”€â”€ memory/                 # Short-term, long-term, episodic memory
â”‚   â”œâ”€â”€ eval/                   # Accuracy, hallucination, bias evaluation
â”‚   â””â”€â”€ llm_router.py           # LLM provider routing with fallback chain
â”‚
â”œâ”€â”€ orchestration/
â”‚   â”œâ”€â”€ graph.py                # LangGraph StateGraph definition
â”‚   â”œâ”€â”€ router.py               # Stage-based routing logic
â”‚   â””â”€â”€ state.py                # DocuForgeState TypedDict
â”‚
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ main.py                 # FastAPI app with lifespan
â”‚   â”œâ”€â”€ routes/                 # analysis_router with all endpoints
â”‚   â””â”€â”€ workers/                # Celery app + analysis task
â”‚
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ v1/writer_prompt.txt    # Basic â€” minimal instructions
â”‚   â”œâ”€â”€ v2/writer_prompt.txt    # Structured â€” JSON schema
â”‚   â””â”€â”€ v3/writer_prompt.txt    # Advanced â€” ReAct + anti-hallucination
â”‚
â”œâ”€â”€ frontend/src/
â”‚   â”œâ”€â”€ App.jsx                 # Main shell with tab state management
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ UploadView.jsx      # File drop + query + prompt version selector
â”‚   â”‚   â”œâ”€â”€ ProcessingView.jsx  # Live agent graph + execution log
â”‚   â”‚   â”œâ”€â”€ ResultsView.jsx     # 4-tab results (Report/ReAct/Prompts/Metrics)
â”‚   â”‚   â””â”€â”€ EvalDashboard.jsx   # Evaluation history and trends
â”‚
â”œâ”€â”€ tests/                      # pytest suite (70%+ coverage)
â”œâ”€â”€ eval/                       # Golden dataset + benchmark results
â”œâ”€â”€ config/docuforge_config.yaml
â”œâ”€â”€ run.py                      # Single-command launcher
â”œâ”€â”€ Makefile                    # Dev shortcuts
â””â”€â”€ .env.example                # Environment template
```

---

## How It Works

### 1. Document Ingestion
Upload a PDF, image, audio file, spreadsheet, or presentation. The Ingestion Agent detects the format and routes to the appropriate parser â€” PyMuPDF for PDFs, Gemini Vision for images, Whisper for audio, pandas for Excel, python-pptx for presentations.

### 2. RAG Pipeline
The RAG Agent chunks the document into 600-character segments, embeds them using local sentence-transformers (no API cost), stores them in an in-memory ChromaDB collection, and retrieves the top-12 most relevant chunks using hybrid BM25 + semantic search.

### 3. Analysis
The Analyst Agent scans the retrieved chunks for numerical data. If found, it generates Python analysis code and executes it in a sandboxed subprocess, extracting key metrics.

### 4. Report Generation
The Writer Agent receives up to 7000 characters of grounded context (ranked chunks + full document text) and a versioned prompt template. The v3 prompt enforces 7 anti-hallucination rules, requiring every sentence to trace back to the document context with an evidence quote.

### 5. Verification
The Verifier Agent extracts 3 specific claims from the generated report and checks each one individually against the source document chunks, returning a per-claim faithfulness array averaged into a final score.

---

## Prompt Engineering

Three prompt versions are available and selectable in the UI before each analysis:

| Version | Strategy | Use Case |
|---|---|---|
| **v1** | Minimal instructions, basic JSON | Quick prototyping |
| **v2** | JSON schema + grounding rules | Standard analysis |
| **v3** | ReAct framework + 7 anti-hallucination rules + evidence fields | Production / interviews |

The selected version is logged in the agent trace and visible in the Prompt Versions tab of the results.

---

## Development

```bash
# Start individual services
make dev         # FastAPI backend only
make worker      # Celery worker only
make frontend    # React frontend only

# Quality checks
make lint        # Ruff linter
make typecheck   # mypy type checking
make test        # pytest with 70% coverage requirement
make check       # Run all checks (lint + typecheck + test)

# Evaluation
make eval        # Run evaluation suite
make benchmark   # Run performance benchmarks

# Cleanup
make clean       # Remove all build artifacts
```

---

## Evaluation

The project includes a comprehensive evaluation framework:

```bash
# Run full eval suite
make eval

# Run benchmark (latency per agent)
make benchmark

# Run specific test file
pytest tests/agents/test_writer_agent.py -v
```

Evaluation covers:
- **Accuracy** â€” LLM-as-judge scoring against golden QA dataset (20 pairs)
- **Faithfulness** â€” Unsupported claim detection against source documents
- **Bias** â€” Response consistency across demographic variants (10 test pairs)
- **Latency** â€” Per-agent benchmark (5 runs each)

---

## Interview Talking Points

**On multi-agent architecture:**
> "Every agent communicates exclusively through a shared DocuForgeState TypedDict â€” no direct imports between agents. This means any agent can be replaced or upgraded independently without touching the rest of the pipeline."

**On hallucination control:**
> "The Verifier Agent doesn't ask the LLM to rate the whole report â€” it extracts 3 specific claims and checks each one individually against source chunks, returning a per-claim support score. The writer prompt has 7 numbered rules that explicitly forbid using general knowledge."

**On RAG design:**
> "Rather than storing in a persistent database, each session gets its own in-memory ChromaDB collection that's destroyed after the task. This prevents cross-session contamination and keeps storage costs at zero."

**On prompt versioning:**
> "Three prompt versions are tracked like software versions. v3 adds the ReAct framework and anti-hallucination rules. The version used is logged in the agent trace so you can reproduce any result exactly."

**On observability:**
> "Every agent appends to a shared agent_trace list. The frontend polls this and renders it as a ReAct Thought/Action/Observation timeline. In production you'd also have LangSmith tracing enabled for token-level visibility."

---

## Cost

| Component | Provider | Cost |
|---|---|---|
| LLM inference | Groq LLaMA 3.1 8B | Free tier |
| Embeddings | HuggingFace all-MiniLM-L6-v2 | Local, free |
| Vector store | ChromaDB in-memory | Free |
| Task queue | Upstash Redis | Free tier (500k ops/month) |
| Image analysis | Google Gemini 1.5 Flash | Free tier |
| Audio transcription | Whisper base | Local, free |

**Total monthly cost: $0**

---

## Performance

- **End-to-end pipeline:** ~45-60 seconds (full analysis with verification)
- **RAG retrieval:** ~1-2 seconds (hybrid search over 50-100 chunks)
- **Report generation:** ~8-12 seconds (writer + verifier LLM calls)
- **Token usage:** ~2,500-3,500 tokens per analysis (Groq free tier: 15 requests/minute)

---

## Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-thing`)
3. Make your changes
4. Run tests and linting (`make check`)
5. Commit with clear messages
6. Push to your fork
7. Open a Pull Request

---

## License

MIT License â€” see [LICENSE](LICENSE) for details.

---

## Acknowledgments

Built with:
- [LangChain](https://langchain.com) â€” Agent framework
- [LangGraph](https://langchain-ai.github.io/langgraph) â€” Orchestration
- [Groq](https://groq.com) â€” Free LLM inference
- [ChromaDB](https://www.trychroma.com/) â€” Vector database
- [FastAPI](https://fastapi.tiangolo.com) â€” Web framework
- [React](https://react.dev) â€” Frontend framework

---

**Live Demo:** https://youtu.be/iMASmk7Rky8  
**Questions?** Open an issue or reach out on LinkedIn!

